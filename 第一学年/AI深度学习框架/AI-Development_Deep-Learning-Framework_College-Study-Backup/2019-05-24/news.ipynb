{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fenci1(s):\n",
    "    cut=jieba.cut(s)\n",
    "    text= ' '.join(cut)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "# import fenci1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.667 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据一共有2条新闻\n",
      "测试数据一共有2条新闻\n",
      "0\n",
      "雨 的 成因 多种多样 它 的 表现 形态 也 各具特色 有 毛毛细雨 ， 有 连绵不断 的 阴雨 ， 还有 倾盆 而 下 的 阵雨 。 地球 上 的 水 受到 太阳光 的 照射 后 ， 就 变成 水蒸气 被 蒸发 到 空气 中去 了 。 水汽 在 高空 遇到 冷空气 便 凝聚 成 小水滴 。 这些 小水滴 都 很小 ， 直径 只有 0.01 ～ 0.02 毫米 ， 最大 也 只有 0.2 毫米 。 它们 又 小 又 轻 ， 被 空气 中 的 上升 气流 托 在 空中 。 就是 这些 小水滴 在 空中 聚成 了 云 。 这些 小水滴 要 变成 雨滴 降到 地面 ， 它 的 体积 大约 要 增大 100 多 万倍 。 这些 小水滴 是 怎样 使 自己 的 体积 增长 到 100 多 万倍 的 呢 ？ 它 主要 依靠 两个 手段 , 其一 是 凝结 和 凝华 增大 。 其二 是 依靠 云滴 的 碰 并 增大 。 在 雨滴 形成 的 初期 ， 云滴 主要 依靠 不断 吸收 云体 四周 的 水气 来 使 自己 凝结 和 凝华 。 如果 云 体内 的 水气 能 源源不断 得到 供应 和 补充 ， 使 云滴 表面 经常 处于 过 饱和状态 ， 那么 ， 这种 凝结 过程 将会 继续下去 ， 使 云滴 不断 增大 ， 成为 雨滴 。 但 有时 云内 的 水气 含量 有限 ， 在 同 一块 云里 ， 水气 往往 供不应求 ， 这样 就 不 可能 使 每个 云滴 都 增大 为 较大 的 雨滴 ， 有些 较 小 的 云滴 只好 归并到 较大 的 云滴 中 去 。 如果 云内 出现 水滴 和 冰晶 共存 的 情况 ， 那么 ， 这种 凝结 和 凝华 增大 过程 将 大大 加快 。 当 云中 的 云滴 增大 到 一定 程度 时 ， 由于 大 云滴 的 体积 和 重量 不断 增加 ， 它们 在 下降 过程 中 不仅 能 赶上 那些 速度 较慢 的 小 云滴 ， 而且 还会 “ 吞并 ” 更 多 的 小 云滴 而 使 自己 壮大 起来 。 当大 云滴 越长越 大 ， 最后 大到 空气 再也 托 不住 它 时 ， 便 从 云中 直落到 地面 ， 成为 我们 常见 的 雨水 。 除了 酸雨 ， 有 颜色 的 雨外 ， 还有 许多 有趣 的 雨 ， 比如 蛙雨 ， 铁雨 ， 金雨 ， 甚至 钱雨 。 它们 都 是 龙卷风 的 杰作 。\n",
      "*********successs to finish fenci readNewsData*************\n"
     ]
    }
   ],
   "source": [
    "print('*************start to fenci***************')\n",
    "newstype={'体育':0,'娱乐':1,'家居':2,'房产':3,'教育':4,'时尚':5,\"时政\":6,\"游戏\":7,'科技':8,'财经':9}\n",
    "fp=open('cnew.train.txt')\n",
    "train_label=[]\n",
    "train_content=[]\n",
    "test_label=[]\n",
    "test_content=[]\n",
    "n=0\n",
    "while True:\n",
    "    cur_text=fp.readline()\n",
    "#     print(cur_text)\n",
    "    if cur_text=='':\n",
    "        break\n",
    "    item=cur_text.split()\n",
    "#     print(item)\n",
    "    train_label.append(newstype[item[0]])\n",
    "    tmp=''.join(item[1:])\n",
    "    text=fenci1(tmp)\n",
    "    train_content.append(text)\n",
    "    n=n+1\n",
    "print(\"训练数据一共有\"+str(n)+\"条新闻\")\n",
    "\n",
    "n=0\n",
    "file_tl=open('rain_label.txt','w')\n",
    "file_tl.write(str(train_label))\n",
    "file_tl.close()\n",
    "file_tc=open('train_content.txt','w')\n",
    "file_tc.write(str(train_content));\n",
    "file_tc.close()\n",
    "\n",
    "fp=open('cnews.test.txt')\n",
    "while True:\n",
    "    cur_text=fp.readline()\n",
    "    if cur_text=='':\n",
    "        break\n",
    "    item=cur_text.split()\n",
    "    test_label.append(newstype[item[0]])\n",
    "    tmp=''.join(item[1:])\n",
    "    text=fenci1(tmp)\n",
    "    test_content.append(text)\n",
    "    n=n+1\n",
    "file_testc=open('test_content.txt','w')\n",
    "file_testc.write(str(test_content));\n",
    "file_testc.close()\n",
    "        \n",
    "file_testl=open('test_content.txt','w')\n",
    "file_testl.write(str(test_label));\n",
    "file_testl.close()\n",
    "print('测试数据一共有'+str(n)+\"条新闻\")\n",
    "\n",
    "print(train_label[0])\n",
    "print(train_content[0])\n",
    "print('*********successs to finish fenci readNewsData*************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[7, 2, 99, 100, 15, 2, 101, 102, 19, 103, 20, 104, 1, 20, 105, 2, 106, 1, 32, 107, 33, 58, 2, 108, 3, 59, 109, 2, 21, 110, 111, 2, 112, 60, 1, 34, 22, 35, 61, 36, 23, 16, 113, 12, 3, 114, 8, 115, 116, 117, 62, 118, 63, 9, 3, 17, 9, 37, 119, 1, 120, 64, 38, 121, 122, 38, 123, 65, 1, 124, 19, 64, 38, 125, 65, 3, 24, 66, 25, 66, 126, 1, 61, 16, 10, 2, 39, 127, 40, 8, 67, 3, 128, 17, 9, 8, 67, 129, 12, 41, 3, 17, 9, 42, 22, 18, 130, 68, 1, 15, 2, 43, 131, 42, 11, 69, 44, 70, 3, 17, 9, 5, 132, 13, 45, 2, 43, 133, 23, 69, 44, 70, 2, 134, 135, 15, 46, 47, 136, 137, 138, 5, 26, 6, 48, 11, 3, 139, 5, 47, 4, 2, 140, 141, 11, 3, 8, 18, 49, 2, 142, 1, 4, 46, 47, 50, 143, 144, 145, 2, 27, 146, 13, 45, 26, 6, 48, 3, 71, 41, 147, 2, 27, 72, 148, 149, 150, 6, 151, 1, 13, 4, 73, 152, 153, 154, 155, 1, 74, 1, 75, 26, 51, 156, 157, 1, 13, 4, 50, 11, 1, 76, 18, 3, 77, 158, 78, 2, 27, 159, 160, 1, 8, 161, 162, 79, 1, 27, 163, 164, 1, 165, 34, 166, 167, 13, 168, 4, 37, 11, 52, 80, 2, 18, 1, 169, 170, 25, 2, 4, 171, 172, 80, 2, 4, 10, 173, 3, 71, 78, 174, 53, 6, 175, 176, 2, 177, 1, 74, 1, 75, 26, 6, 48, 11, 51, 178, 179, 180, 3, 81, 28, 2, 4, 11, 23, 82, 181, 83, 1, 182, 54, 4, 2, 43, 6, 183, 50, 184, 1, 24, 8, 185, 51, 10, 186, 72, 187, 188, 189, 190, 2, 25, 4, 1, 191, 192, 193, 194, 195, 196, 44, 2, 25, 4, 33, 13, 45, 197, 198, 3, 199, 4, 200, 54, 1, 201, 84, 16, 202, 40, 85, 15, 83, 1, 62, 29, 28, 203, 68, 1, 76, 204, 205, 2, 14, 3, 206, 86, 1, 20, 207, 2, 208, 1, 32, 209, 210, 2, 7, 1, 211, 212, 1, 213, 1, 214, 1, 215, 216, 3, 24, 37, 5, 217, 2, 218, 3]\n",
      "388\n",
      "65\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "token = Tokenizer(num_words=3000)\n",
    "token.fit_on_texts(train_content)\n",
    "token.fit_on_texts(test_content)\n",
    "\n",
    "print(token.document_count)\n",
    "\n",
    "trainSqe = token.texts_to_sequences(train_content)\n",
    "testSqe = token.texts_to_sequences(test_content)\n",
    "\n",
    "print(trainSqe[0])\n",
    "\n",
    "trainPadSqe = sequence.pad_sequences(trainSqe,maxlen=128)\n",
    "testPadSqe = sequence.pad_sequences(testSqe,maxlen=128)\n",
    "\n",
    "print(len(trainSqe[0]))\n",
    "print(len(testSqe[0]))\n",
    "\n",
    "train_label_ohe=np_utils.to_categorical(train_label)\n",
    "test_label_ohe=np_utils.to_categorical(test_label)\n",
    "\n",
    "print(train_label_ohe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 128, 32)           96000     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               8448      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 106,785\n",
      "Trainable params: 106,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=32,input_dim=3000,input_length=128))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "model.add(SimpleRNN(units=32))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=256,activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(units=10,activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "没有模型加载，开始训练新模型\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights('newstpye.h5')\n",
    "    print(\"成功加载已有模型，继续训练该模型\")\n",
    "except:\n",
    "    print(\"没有模型加载，开始训练新模型\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (2, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-08c65494d821>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainPadSqe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_label_ohe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"newstpye.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;31m# using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 check_loss_and_target_compatibility(\n\u001b[0;32m--> 809\u001b[0;31m                     y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 raise ValueError(\n\u001b[1;32m    272\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (2, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "    \n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['categorical_accuracy'])\n",
    "train_history=model.fit(x=trainPadSqe,y=train_label_ohe,validation_split=0.2,epochs=5,batch_size=100,verbose=1)\n",
    "\n",
    "model.save_weights(\"newstpye.h5\")\n",
    "print(\"保存刚训练的模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have shape (10,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f6fbfe41f6b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestPadSqe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_label_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc='\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredic_pro\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestPadSqe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mshow_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'the news label is :'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have shape (10,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "results=model.evaluate(testPadSqe,test_label_ohe)\n",
    "print('acc=',results[1])\n",
    "predic_pro=model.predict(testPadSqe)\n",
    "def show_categorical(num):\n",
    "    print('the news label is :'+str(test_label[num]))\n",
    "    for j in range(10):\n",
    "        print('newstype'+str(j)+'probility is:%1.9f'%(predic_pro[num][j]))\n",
    "    print(test_content[num])\n",
    "show_categorical(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
